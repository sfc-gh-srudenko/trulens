{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TruLens vs RAGAS vs MLFlow performance comparison for groundedness\n",
    "\n",
    "In this notebook, we analyze the performance of TruLens current groundedness feedback function and its comparable or equivalent implementations from other evaluation frameworks using the same model for LLM-as-judges. \n",
    "\n",
    "\n",
    "### Definitions\n",
    "1. TruLens `groundedness`: evaluates whether a response is fully supported by the source or retrieved contexts.\n",
    "\n",
    "2. RAGAS `faithfulness`: measures the factual consistency of the generated answer against the given context [source](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
    "\n",
    "3. MLflow `faithfulness`: Faithfulness will be assessed based on how factually consistent the output is to the context\n",
    "[source](https://mlflow.org/docs/latest/python_api/mlflow.metrics.html#mlflow.metrics.genai.faithfulness)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -q trulens-core trulens-providers-openai ragas mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare 3 public benchmark datasets: QAGS CNN/Daily Mail, QAGS XSum, and SummEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_qags_golden_set_groundedness,\n",
    ")\n",
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    generate_summeval_groundedness_golden_set,\n",
    ")\n",
    "\n",
    "qags_cnn_dm = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_cnndm.jsonl\"))\n",
    ")\n",
    "\n",
    "qags_xsum = pd.DataFrame(\n",
    "    list(generate_qags_golden_set_groundedness(\"data/qags_mturk_xsum.jsonl\"))\n",
    ")\n",
    "\n",
    "summeval = pd.DataFrame(\n",
    "    list(\n",
    "        generate_summeval_groundedness_golden_set(\n",
    "            \"data/summeval_test.json\", max_samples_per_bucket=200\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "summeval_true_labels = [row[\"expected_score\"] for _, row in summeval.iterrows()]\n",
    "\n",
    "summeval_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in summeval_true_labels\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_cnn_dm.iterrows()\n",
    "]\n",
    "\n",
    "qags_cnn_dm_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_cnn_dm_true_labels\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels = [\n",
    "    row[\"expected_score\"] for _, row in qags_xsum.iterrows()\n",
    "]\n",
    "\n",
    "qags_xsum_true_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in qags_xsum_true_labels\n",
    "]\n",
    "combined_dataset = pd.concat(\n",
    "    [qags_cnn_dm, qags_xsum, summeval], ignore_index=True\n",
    ")\n",
    "combined_true_labels = (\n",
    "    qags_cnn_dm_true_labels + qags_xsum_true_labels + summeval_true_labels\n",
    ")\n",
    "\n",
    "assert len(combined_dataset) == len(combined_true_labels)\n",
    "print(f\"Total number of samples: {len(combined_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    visualize_expected_score_distribution,\n",
    ")\n",
    "\n",
    "# making sure the distribution of the expected scores is as balanced as possible for the datasets\n",
    "visualize_expected_score_distribution(combined_true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiments with TruLens `TruBasicApp` recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import mlflow\n",
    "from mlflow.metrics.genai import faithfulness as faithfulness_mlflow\n",
    "from ragas import evaluate\n",
    "from ragas.cost import get_token_usage_for_openai\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.metrics import faithfulness as faithfulness_ragas\n",
    "from trulens.apps.basic import TruBasicApp\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "OPENAI_LLM_NAME = \"gpt-4o-mini\"\n",
    "gpt_4o_mini = OpenAI(model_engine=OPENAI_LLM_NAME)\n",
    "\n",
    "\n",
    "def trulens_groundedness(context: str, response: str, gt_score: float) -> str:\n",
    "    trulens_groundedness_res = (\n",
    "        gpt_4o_mini.groundedness_measure_with_cot_reasons(\n",
    "            source=context, statement=response, use_sent_tokenize=True\n",
    "        )\n",
    "    )\n",
    "    return f\"{trulens_groundedness_res[0]};{gt_score}\"\n",
    "\n",
    "\n",
    "langchain_llm = llm_factory(model=OPENAI_LLM_NAME)\n",
    "faithfulness_mlflow.llm = langchain_llm\n",
    "\n",
    "\n",
    "def ragas_faithfulness(context: str, response: str, gt_score: float) -> str:\n",
    "    data_samples = {\"question\": [], \"answer\": [], \"contexts\": []}\n",
    "    data_samples[\"question\"].append(\"dummy text\")\n",
    "    data_samples[\"answer\"].append(response)\n",
    "    data_samples[\"contexts\"].append(context)\n",
    "    ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "    score_dict = evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[faithfulness_ragas],\n",
    "        llm=langchain_llm,\n",
    "        token_usage_parser=get_token_usage_for_openai,\n",
    "    )\n",
    "\n",
    "    return f\"{score_dict['faithfulness']};{gt_score}\"\n",
    "\n",
    "\n",
    "faithfulness_metric = faithfulness_mlflow(\n",
    "    model=f\"openai:/{OPENAI_LLM_NAME}\"\n",
    ")  # not supplying any example as other metrics do zero-shot evaluation as well\n",
    "\n",
    "\n",
    "def mlflow_faithfulness(context: str, response: str, gt_score: float) -> str:\n",
    "    eval_data = pd.DataFrame({\n",
    "        \"inputs\": [\n",
    "            \"dummy text\"  # we are not using the inputs (user's queries) for faithfulness evaluation\n",
    "        ],\n",
    "        \"predictions\": [response],\n",
    "        \"context\": [context],\n",
    "    })\n",
    "\n",
    "    with mlflow.start_run() as _:\n",
    "        results = mlflow.evaluate(\n",
    "            data=eval_data,\n",
    "            predictions=\"predictions\",\n",
    "            extra_metrics=[\n",
    "                faithfulness_metric,\n",
    "            ],\n",
    "            evaluators=\"default\",\n",
    "        )\n",
    "\n",
    "    mlflow_faithfulness_score = results.metrics[\"faithfulness/v1/mean\"]\n",
    "\n",
    "    mlflow_faithfulness_score_norm = (\n",
    "        mlflow_faithfulness_score - 1\n",
    "    ) / 4.0  # normalizing the score to be between 0 and 1\n",
    "\n",
    "    return f\"{mlflow_faithfulness_score_norm};{gt_score}\"\n",
    "\n",
    "\n",
    "def run_experiment_and_record(\n",
    "    evaluate_func_wrapper, app_name, app_version, dataset_df, true_labels\n",
    "):\n",
    "    if len(dataset_df) != len(true_labels):\n",
    "        raise ValueError(\"dataset df must have the same length as labels\")\n",
    "\n",
    "    tru_wrapped_basic_app = TruBasicApp(\n",
    "        evaluate_func_wrapper, app_name=app_name, app_version=app_version\n",
    "    )\n",
    "\n",
    "    for i in range(len(dataset_df)):\n",
    "        arg_1 = dataset_df.iloc[i][\"query\"]\n",
    "        arg_2 = dataset_df.iloc[i][\"expected_response\"]\n",
    "        arg_3 = true_labels[i]\n",
    "\n",
    "        try:\n",
    "            with tru_wrapped_basic_app as _:\n",
    "                tru_wrapped_basic_app.app(arg_1, arg_2, arg_3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error {e} in run_feedback_experiment row {i} with first arg {arg_1} and second arg {arg_2}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_and_record(\n",
    "    evaluate_func_wrapper=trulens_groundedness,\n",
    "    app_name=\"trulens-groundedness\",\n",
    "    app_version=\"10302024\",\n",
    "    dataset_df=combined_dataset,\n",
    "    true_labels=combined_true_labels,\n",
    ")\n",
    "\n",
    "run_experiment_and_record(\n",
    "    evaluate_func_wrapper=ragas_faithfulness,\n",
    "    app_name=\"ragas-faithfulness\",\n",
    "    app_version=\"10302024\",\n",
    "    dataset_df=combined_dataset,\n",
    "    true_labels=combined_true_labels,\n",
    ")\n",
    "\n",
    "run_experiment_and_record(\n",
    "    evaluate_func_wrapper=mlflow_faithfulness,\n",
    "    app_name=\"mlflow-faithfulness\",\n",
    "    app_version=\"10302024\",\n",
    "    dataset_df=combined_dataset,\n",
    "    true_labels=combined_true_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about column name mapping: in all our dataframes (CNN/DM, XSUM, and SummEval), the \"expected_score\" column is the ground truth (true) label for the groundedness score, query corresponds to the context, and expected_response corresponds to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking with real-valued output scores (both TruLens' feedback scores and RAGAS scores are normalized to 0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    read_results,\n",
    ")\n",
    "\n",
    "trulens_cnn_scores, cnn_labels, latencies = read_results(\n",
    "    \"results/QAGS CNN_DM - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores, xsum_labels, latencies = read_results(\n",
    "    \"results/QAGS XSum - gpt-4o-mini_groundedness_likert4_results.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking with binary output scores (both TruLens' feedback scores and RAGAS scores are cast to 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens.benchmark.benchmark_frameworks.experiments.dataset_preprocessing import (\n",
    "    compute_binary_classification_metrics,\n",
    ")\n",
    "from trulens.feedback.groundtruth import GroundTruthAggregator\n",
    "\n",
    "trulens_cnn_dm_scores, trulens_cnn_dm_labels, trulens_cnn_dm_latencies = (\n",
    "    read_results(\"results/groundedness-binary-10102024_qags-cnn-dm_results.csv\")\n",
    ")\n",
    "trulens_cnn_dm_scores_binary = [\n",
    "    1 if score >= 0.5 else 0 for score in trulens_cnn_dm_scores\n",
    "]\n",
    "trulens_cnn_dm_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in trulens_cnn_dm_labels\n",
    "]\n",
    "print(len(trulens_cnn_dm_scores_binary), len(trulens_cnn_dm_labels_binary))\n",
    "\n",
    "spearman_cor = GroundTruthAggregator(\n",
    "    trulens_cnn_dm_labels\n",
    ").spearman_correlation(trulens_cnn_dm_scores)\n",
    "\n",
    "compute_binary_classification_metrics(\n",
    "    \"TruLens QAGS CNN/Daily Mail\",\n",
    "    trulens_cnn_dm_labels_binary,\n",
    "    trulens_cnn_dm_scores_binary,\n",
    "    trulens_cnn_dm_latencies,\n",
    ")\n",
    "print(f\"TruLens QAGS CNN/Daily Mail: {spearman_cor}\")\n",
    "\n",
    "\n",
    "trulens_xsum_scores, xsum_labels, trulens_xsum_latencies = read_results(\n",
    "    \"results/groundedness-binary-10102024_qags-xsum_results.csv\"\n",
    ")\n",
    "trulens_xsum_scores_binary = [\n",
    "    1 if score >= 0.5 else 0 for score in trulens_xsum_scores\n",
    "]\n",
    "trulens_xsum_labels_binary = [1 if label >= 0.5 else 0 for label in xsum_labels]\n",
    "spearman_cor = GroundTruthAggregator(xsum_labels).spearman_correlation(\n",
    "    trulens_xsum_scores\n",
    ")\n",
    "\n",
    "print(len(trulens_xsum_scores_binary), len(trulens_xsum_labels_binary))\n",
    "\n",
    "compute_binary_classification_metrics(\n",
    "    \"TruLens QAGS XSum\",\n",
    "    trulens_xsum_labels_binary,\n",
    "    trulens_xsum_scores_binary,\n",
    "    trulens_xsum_latencies,\n",
    ")\n",
    "print(f\"TruLens QAGS XSum: {spearman_cor}\")\n",
    "\n",
    "trulens_summeval_scores, summeval_labels, trulens_summeval_latencies = (\n",
    "    read_results(\n",
    "        \"results/groundedness-binary-10102024_summeval-subset_results.csv\"\n",
    "    )\n",
    ")\n",
    "trulens_summeval_binary = [\n",
    "    1 if score >= 0.5 else 0 for score in trulens_summeval_scores\n",
    "]\n",
    "trulens_summeval_labels_binary = [\n",
    "    1 if label >= 0.5 else 0 for label in summeval_labels\n",
    "]\n",
    "print(len(trulens_summeval_binary), len(trulens_summeval_labels_binary))\n",
    "spearman_cor = GroundTruthAggregator(summeval_labels).spearman_correlation\n",
    "\n",
    "compute_binary_classification_metrics(\n",
    "    \"TruLens SummEval subset\",\n",
    "    trulens_summeval_labels_binary,\n",
    "    trulens_summeval_binary,\n",
    "    trulens_summeval_latencies,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trulens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
